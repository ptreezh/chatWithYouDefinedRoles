# 🚀 Chat4 启动和测试指南

## 📋 系统要求

### 必需软件
- **Node.js**: 版本 18.0 或更高
- **npm**: 包管理器
- **Ollama**: 本地AI模型服务

### 推荐模型
- `llama2` - 通用对话模型
- `mistral` - 高性能模型
- `codellama` - 编程专用模型

## 🛠️ 安装和配置

### 1. 安装Ollama
```bash
# Windows (推荐使用WSL2或PowerShell)
iwr -useb https://ollama.ai/install.ps1 | iex

# 或者手动下载: https://ollama.ai/download
```

### 2. 下载模型
```bash
# 启动Ollama服务
ollama serve

# 在新终端中下载模型
ollama pull llama2
ollama pull mistral

# 验证模型
ollama list
```

### 3. 设置项目
```bash
# 进入项目目录
cd Chat4

# 安装依赖
npm install

# 检查环境
node scripts/simple-check.js
```

## 🚀 启动步骤

### 方法1: 分别启动服务

#### 步骤1: 启动Ollama
```bash
# 在第一个终端中
ollama serve
```

#### 步骤2: 启动开发服务器
```bash
# 在第二个终端中
cd Chat4
npm run dev
```

#### 步骤3: 验证服务
```bash
# 在第三个终端中
cd Chat4
npm run test:basic
```

### 方法2: 自动化启动

```bash
# 使用启动脚本
cd Chat4
node scripts/startup.js
```

## 🌐 访问应用

### Web界面
```
http://localhost:3000
```

### API端点
- 健康检查: `http://localhost:3000/api/health`
- 角色管理: `http://localhost:3000/api/characters`
- 主题管理: `http://localhost:3000/api/themes`

## 🧪 运行测试

### 基础测试
```bash
# 环境检查
npm run test:basic

# API连接测试
npm run test:api

# 健康检查
npm run test:health
```

### 功能测试
```bash
# 端到端聊天测试
npm run test:e2e

# 性能测试
npm run test:performance

# 完整测试套件
npm run test:all
```

### 高级测试
```bash
# 选择性测试
node scripts/test-runner.js --e2e --performance

# 自定义配置
TEST_TIMEOUT=60000 node scripts/test-runner.js
```

## 🎯 实际使用验证

### 1. 界面功能验证
- [ ] 主页面加载正常
- [ ] 可以看到角色列表
- [ ] 主题管理面板可用
- [ ] API配置界面显示

### 2. 角色管理验证
- [ ] 可以上传角色文件 (.txt, .json, .md)
- [ ] 角色文件正确解析
- [ ] 角色显示在列表中
- [ ] 可以激活/停用角色

### 3. 主题管理验证
- [ ] 可以创建新主题
- [ ] 可以为主题上传角色
- [ ] 主题切换正常工作
- [ ] 角色按主题过滤

### 4. 聊天功能验证
- [ ] 可以发送消息
- [ ] 角色生成回复
- [ ] 多角色参与对话
- [ ] 对话历史保存

### 5. 本地LLM验证
- [ ] 实际调用Ollama模型
- [ ] 回复内容合理
- [ ] 响应时间可接受 (2-5秒)
- [ ] 支持不同模型

## 📊 预期结果

### 成功指标
- ✅ 所有基础测试通过
- ✅ 端到端测试成功率 > 90%
- ✅ 本地LLM调用正常工作
- ✅ 响应时间 < 5秒
- ✅ 界面功能完整

### 测试报告示例
```
🎯 测试总结
总测试数: 6
通过测试: 5
失败测试: 1
成功率: 83.3%

📊 性能指标
平均响应时间: 2.45秒
最大吞吐量: 12.5 req/s
成功率: 95.2%

🤖 本地模型状态
Ollama服务: 正常
可用模型: llama2, mistral
生成测试: 成功
```

## 🔧 故障排除

### 常见问题

#### 1. Ollama连接失败
```bash
# 检查Ollama服务
curl http://localhost:11434/api/tags

# 重启Ollama
# Windows: 重启Ollama应用
# Linux/Mac: sudo systemctl restart ollama
```

#### 2. 端口冲突
```bash
# 检查端口占用
netstat -an | grep 3000

# 更改端口 (在server.ts中)
const currentPort = 3001;
```

#### 3. 依赖问题
```bash
# 清理并重新安装
rm -rf node_modules package-lock.json
npm install
```

#### 4. 数据库问题
```bash
# 重置数据库
npm run db:reset

# 重新生成Prisma客户端
npm run db:generate
```

### 调试技巧

#### 启用详细日志
```bash
# 查看开发服务器日志
tail -f dev.log

# 启用调试模式
DEBUG=* npm run dev
```

#### 检查网络连接
```bash
# 测试Ollama连接
curl http://localhost:11434/api/tags

# 测试应用健康
curl http://localhost:3000/api/health
```

#### 验证文件结构
```bash
# 运行环境检查
node scripts/simple-check.js
```

## 📈 性能优化

### 1. Ollama优化
```bash
# 使用更小的模型
ollama pull tinyllama

# 调整参数 (在应用配置中)
{
  "temperature": 0.5,
  "maxTokens": 500
}
```

### 2. 系统优化
```bash
# 增加Node.js内存
export NODE_OPTIONS="--max-old-space-size=4096"

# 使用PM2管理进程
npm install -g pm2
pm2 start npm --name "chat4" -- run dev
```

### 3. 数据库优化
```bash
# 定期清理
npm run db:reset

# 索引优化 (在Prisma schema中)
model Character {
  // 添加索引
  @@index([theme])
  @@index([category])
}
```

## 🎉 成功标准

### 完整功能验证
- [ ] 用户可以创建主题和上传角色
- [ ] 系统实际调用本地Ollama模型
- [ ] 多角色对话正常工作
- [ ] 主题切换和角色过滤正常
- [ ] 所有自动化测试通过

### 性能标准
- [ ] 页面加载时间 < 3秒
- [ ] API响应时间 < 1秒
- [ ] LLM生成时间 < 5秒
- [ ] 并发支持 > 10用户
- [ ] 系统稳定性 > 99%

### 用户体验
- [ ] 界面直观易用
- [ ] 功能响应及时
- [ ] 错误处理友好
- [ ] 文档完整清晰

---

**提示**: 本系统专为本地Ollama模型优化，确保按照指南正确配置环境以获得最佳体验。